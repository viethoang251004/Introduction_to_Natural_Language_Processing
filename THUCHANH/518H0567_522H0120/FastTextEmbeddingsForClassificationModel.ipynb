{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01tPaclmtfky"
      },
      "source": [
        "# Step 1: Install and Load FastText\n",
        "First, install the fasttext library and download a pretrained FastText model if you haven't done so already:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVNEppOPrkeg",
        "outputId": "ac2b9802-86af-4e74-e650-f1d04b48540a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.10/dist-packages (0.9.3)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from fasttext) (2.13.6)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (75.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install fasttext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXBa7xpGtnGa"
      },
      "source": [
        "Load the pretrained FastText model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qGrrjpJztFfz"
      },
      "outputs": [],
      "source": [
        "import fasttext\n",
        "import fasttext.util\n",
        "\n",
        "# Load the pretrained FastText English embeddings\n",
        "# fasttext.util.download_model('en', if_exists='ignore')  # Replace 'en' with 'vi' for Vietnamese\n",
        "ft = fasttext.load_model('cc.en.300.bin')  # Load 300-dimensional FastText English embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpSj7VKot3YE"
      },
      "source": [
        "# Step 2: Modify the load_data and Embedding Extraction\n",
        "Replace the BERT tokenization and embeddings in the Question 1 with FastText embeddings for each word in the sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cgXSSF-ZuVap"
      },
      "outputs": [],
      "source": [
        "import fasttext\n",
        "import fasttext.util\n",
        "\n",
        "# Load the FastText model for Vietnamese\n",
        "# fasttext.util.download_model('en', if_exists='ignore')  # Only runs if the model is not already downloaded\n",
        "# ft = fasttext.load_model('cc.en.300.bin')  # Adjust the path if needed\n",
        "\n",
        "# Define the function to load data\n",
        "def load_data(filepath):\n",
        "    sentences, labels = [], []\n",
        "    with open(filepath, 'r') as f:\n",
        "        sentence, label = [], []\n",
        "        for line in f:\n",
        "            if line.startswith('-DOCSTART-') or line == \"\\n\":\n",
        "                if sentence:\n",
        "                    sentences.append(sentence)\n",
        "                    labels.append(label)\n",
        "                sentence, label = [], []\n",
        "            else:\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) == 4:  # Only process lines with exactly four parts\n",
        "                    word, _, _, tag = parts\n",
        "                    sentence.append(word)\n",
        "                    label.append(tag)\n",
        "        if sentence:\n",
        "            sentences.append(sentence)\n",
        "            labels.append(label)\n",
        "    return sentences, labels\n",
        "\n",
        "# Load sentences and labels as before\n",
        "train_sentences, train_labels = load_data('/content/drive/MyDrive/Colab_Notebooks/MIDTERM_NLP/train.txt')\n",
        "test_sentences, test_labels = load_data('/content/drive/MyDrive/Colab_Notebooks/MIDTERM_NLP/test.txt')\n",
        "valid_sentences, valid_labels = load_data('/content/drive/MyDrive/Colab_Notebooks/MIDTERM_NLP/valid.txt')\n",
        "\n",
        "# Convert sentences to FastText embeddings\n",
        "def get_fasttext_embeddings(sentences):\n",
        "    embedded_sentences = []\n",
        "    for sentence in sentences:\n",
        "        sentence_embeddings = [ft.get_word_vector(word) for word in sentence]\n",
        "        embedded_sentences.append(sentence_embeddings)\n",
        "    return embedded_sentences\n",
        "\n",
        "# Get embeddings for each dataset\n",
        "train_embeddings = get_fasttext_embeddings(train_sentences)\n",
        "test_embeddings = get_fasttext_embeddings(test_sentences)\n",
        "valid_embeddings = get_fasttext_embeddings(valid_sentences)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETAgv-RBuyb0"
      },
      "source": [
        "# Step 3: Update the NERDataset Class for FastText Embeddings\n",
        "Since FastText embeddings are already in vector form, you won’t need additional token offsets or padding. Each word vector can be directly used as input to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buvuxQiJv2i0"
      },
      "outputs": [],
      "source": [
        "import fasttext\n",
        "import numpy as np\n",
        "\n",
        "# Tải mô hình FastText đã huấn luyện trước cho tiếng Việt\n",
        "fasttext_model = fasttext.load_model('cc.en.300.bin')  # Đường dẫn đến mô hình tiếng Việt đã huấn luyện\n",
        "\n",
        "# Hàm chuyển đổi câu thành vector embedding với FastText\n",
        "def sentence_to_embedding(sentences):\n",
        "    embedded_sentences = []\n",
        "    for sentence in sentences:\n",
        "        embedded_sentence = [fasttext_model.get_word_vector(word) for word in sentence]\n",
        "        embedded_sentences.append(embedded_sentence)\n",
        "    return embedded_sentences\n",
        "\n",
        "# Áp dụng FastText cho các câu train, test, valid\n",
        "train_embeddings = sentence_to_embedding(train_sentences)\n",
        "test_embeddings = sentence_to_embedding(test_sentences)\n",
        "valid_embeddings = sentence_to_embedding(valid_sentences)\n",
        "\n",
        "# Chuyển đổi các embedding và nhãn thành dạng PyTorch Tensor Dataset\n",
        "import torch\n",
        "\n",
        "class FastTextNERDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, embeddings, labels):\n",
        "        self.embeddings = embeddings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': torch.tensor(self.embeddings[idx], dtype=torch.float32),\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Tạo tập dữ liệu huấn luyện, kiểm thử và xác thực\n",
        "train_dataset = FastTextNERDataset(train_embeddings, train_labels)\n",
        "test_dataset = FastTextNERDataset(test_embeddings, test_labels)\n",
        "valid_dataset = FastTextNERDataset(valid_embeddings, valid_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HUvwY3FwHpk"
      },
      "source": [
        "# Step 4: Train the Model with Hugging Face Trainer\n",
        "Since we’re using a custom model, we can train it directly with the Hugging Face Trainer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "FGbaFvet9YMF",
        "outputId": "667d549f-4e79-472a-ffc0-d30ca2c58931"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'TrainingArguments' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7f85b696b143>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Define training arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Các tham số huấn luyện được định nghĩa bằng TrainingArguments:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m training_args = TrainingArguments(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Thư mục lưu trữ kết quả huấn luyện\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./results'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'TrainingArguments' is not defined"
          ]
        }
      ],
      "source": [
        "# Define training arguments\n",
        "# Các tham số huấn luyện được định nghĩa bằng TrainingArguments:\n",
        "training_args = TrainingArguments(\n",
        "    # Thư mục lưu trữ kết quả huấn luyện\n",
        "    output_dir='./results',\n",
        "    # evaluation_strategy=\"epoch\" chỉ định việc đánh giá vào cuối mỗi epoch\n",
        "    #(một lần duyệt qua toàn bộ dữ liệu huấn luyện).\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    # learning_rate=2e-5: Tốc độ học của mô hình, xác định mức độ điều chỉnh trọng số sau mỗi lần cập nhật.\n",
        "    learning_rate=2e-5,\n",
        "    # Kích thước batch cho mỗi thiết bị trong quá trình huấn luyện.\n",
        "    per_device_train_batch_size=16,\n",
        "    # Kích thước batch cho mỗi thiết bị trong quá trình đánh giá.\n",
        "    per_device_eval_batch_size=16,\n",
        "    # Số lượng epoch để huấn luyện mô hình.\n",
        "    num_train_epochs=3,\n",
        "    # Hệ số giảm trọng số, giúp tránh overfitting bằng cách giảm giá trị của các trọng số lớn.\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "# Define Trainer\n",
        "# Khởi tạo một đối tượng Trainer từ thư viện Transformers của Hugging Face\n",
        "#Trainer một lớp giúp bạn dễ dàng huấn luyện và đánh giá mô hình.\n",
        "trainer = Trainer(\n",
        "    # model=model: Đây là mô hình bạn muốn huấn luyện.\n",
        "    #Nó có thể là bất kỳ mô hình nào từ thư viện Transformers, chẳng hạn như BERT, GPT-2, v.v.\n",
        "    model=model,\n",
        "    # Đây là các tham số huấn luyện mà đã được định nghĩa ở trên,\n",
        "    #chẳng hạn như tốc độ học, số lượng epoch, kích thước batch, v.v.\n",
        "    args=training_args,\n",
        "    # train_dataset=train_dataset là tập dữ liệu huấn luyện dùng để huấn luyện mô hình.\n",
        "    train_dataset=train_dataset,\n",
        "    # eval_dataset=test_dataset là tập dữ liệu đánh giá dùng để đánh giá mô hình trong quá trình huấn luyện.\n",
        "    eval_dataset=test_dataset\n",
        ")\n",
        "\n",
        "# Sử dụng method train() của trainer để train model.\n",
        "# Sau khi train xong, model sẽ sẵn sàng để đánh giá và sử dụng cho các tác vụ dự đoán.\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Evaluation"
      ],
      "metadata": {
        "id": "JyLppcF2e6tQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Evaluate the model on the validation dataset\n",
        "# Mô hình được đánh giá trên tập valid_dataset bằng cách sử dụng trainer.evaluate().\n",
        "#Nó trả về các chỉ số như độ chính xác và độ mất mát trên valid_dataset.\n",
        "# evaluation_results = trainer.evaluate(eval_dataset=valid_dataset)\n",
        "# in ra {'eval_loss': 0.021987076848745346,\n",
        "#'eval_runtime': 25.482,\n",
        "#'eval_samples_per_second': 127.541,\n",
        "#'eval_steps_per_second': 8.006,\n",
        "#'epoch': 3.0}\n",
        "# print(evaluation_results)\n",
        "\n",
        "# Get predictions on the test set\n",
        "# method trainer.predict() được sử dụng để lấy các dự đoán trên tập test_dataset\n",
        "#labels là ID nhãn thực từ tập test_dataset.\n",
        "predictions, labels, _ = trainer.predict(test_dataset)\n",
        "# predictions chứa các điểm số thô cho mỗi lớp,\n",
        "#được chuyển đổi thành ID nhãn bằng cách lấy argmax.\n",
        "predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "# Define a function to convert the label IDs back to label names\n",
        "def convert_labels(predictions, labels, label_list):\n",
        "    pred_list = []\n",
        "    true_list = []\n",
        "    for pred, label in zip(predictions, labels):\n",
        "        pred_list.append([label_list[p] for p in pred])\n",
        "        true_list.append([label_list[l] for l in label])\n",
        "    return pred_list, true_list\n",
        "\n",
        "# Convert the predictions and labels back to their original format\n",
        "pred_labels, true_labels = convert_labels(predictions, test_labels, label_list)\n",
        "# Generate a classification report\n",
        "from sklearn.metrics import classification_report\n",
        "# Flatten the lists for easier comparison\n",
        "true_labels_flat = [item for sublist in true_labels for item in sublist]\n",
        "pred_labels_flat = [item for sublist in pred_labels for item in sublist]\n",
        "# Generate and print the classification report\n",
        "print(classification_report(true_labels_flat, pred_labels_flat, target_names=label_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "cN4hD1CPeqQ9",
        "outputId": "a25495b5-cece-468f-f12d-ebc3b36399ea"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'trainer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-28acc0e34e17>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# method trainer.predict() được sử dụng để lấy các dự đoán trên tập test_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#labels là ID nhãn thực từ tập test_dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;31m# predictions chứa các điểm số thô cho mỗi lớp,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#được chuyển đổi thành ID nhãn bằng cách lấy argmax.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "9HUvwY3FwHpk",
        "JyLppcF2e6tQ"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}